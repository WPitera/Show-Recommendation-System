{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gradio as gr\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Credentials (Replace with your credentials)\n",
    "CLIENT_ID = \"8d3314772d90b39ef86aeed6ff9868aa\"\n",
    "API_URL = \"https://api.myanimelist.net/v2/anime\"\n",
    "Fields = \"id,title,alternative_titles,synopsis,genres,mean,rank,popularity,media_type,status\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_anime_list(offset=0):\n",
    "    headers = {\"X-MAL-Client-ID\": CLIENT_ID}\n",
    "    params = {\"ranking_type\": \"all\", \"limit\": 100, \"offset\": offset, \"fields\": Fields}\n",
    "    response = requests.get(f\"{API_URL}/ranking\", headers=headers, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"data\", [])\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(anime_list, existing_df):\n",
    "    # Create a DataFrame from the MAL API response\n",
    "    new_data = pd.DataFrame([\n",
    "        {\n",
    "            \"id\": anime[\"node\"][\"id\"],  # Unique MAL ID\n",
    "            \"title\": anime[\"node\"][\"title\"],  # Anime title\n",
    "            \"alternative_titles\": anime[\"node\"][\"title\"],  # Anime title\n",
    "            \"synopsis\": anime[\"node\"].get(\"synopsis\", \"\"),  # Synopsis (if available)\n",
    "            \"genres\": \", \".join([g[\"name\"] for g in anime[\"node\"].get(\"genres\", [])]),  # Genres as a comma-separated string\n",
    "            \"mean\": anime[\"node\"].get(\"mean\", 0),  # Average score, defaulting to 0 if missing\n",
    "            \"media_type\": anime[\"node\"].get(\"media_type\", \"unknown\"),  # Type (TV, movie, OVA, etc.)\n",
    "            \"status\": anime[\"node\"].get(\"status\", \"unknown\")  # Status (e.g., finished_airing, airing)\n",
    "        }\n",
    "        for anime in anime_list\n",
    "    ])\n",
    "    # Merge with existing data if provided\n",
    "    if existing_df is not None:\n",
    "        new_data = pd.concat([existing_df, new_data], ignore_index=True)\n",
    "    return(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None \n",
    "for i in range(0, 21525, 100):\n",
    "    anime_list = fetch_anime_list(i)\n",
    "    df = preprocess_data(anime_list,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('animelist1.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini API\n",
    "GEMINI_API_KEY = 'AIzaSyBewqt6hAd155l__Ot6kgfVtUAKw-iGWqo'\n",
    "GEMINI_MODEL = \"gemini-1.5-flash\"\n",
    "llm = ChatGoogleGenerativeAI(google_api_key=GEMINI_API_KEY, model=GEMINI_MODEL, temperature=0.3)\n",
    "\n",
    "def extract_base_title_with_gemini(title):\n",
    "    \"\"\"\n",
    "    Uses Gemini to intelligently parse and clean anime titles.\n",
    "    \"\"\"\n",
    "    prompt = (f\"Simplify the anime title: '{title}', to only return the series name. Remove season numbers, 'Movie', 'Special', 'OVA', and any similar identifiers. Only return the series name.\")\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Gemini API: {e}\")\n",
    "        return title  # Fallback in case of an error    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You Have two options when running this code: Either Get A Premium Key or Brute Force it By doing it in chunks of 97 per chunk\n",
    "for index, row in df.iloc[0:].iterrows():\n",
    "    #time.sleep(2)\n",
    "    df.at[index, \"title\"] = extract_base_title_with_gemini(row[\"title\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('animelist2.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"animelist2.csv\", encoding=\"latin1\")\n",
    "# Transform it into the desired format\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        \"id\": row[\"id\"],  # Assuming the CSV has an \"id\" column\n",
    "        \"title\": row[\"title\"],  # Assuming the CSV has a \"title\" column\n",
    "        \"alternative_titles\": row[\"alternative_titles\"],  # If missing, set default\n",
    "        \"synopsis\": row.get(\"synopsis\", \"\"),  # Ensure default empty string if missing\n",
    "        \"genres\": row[\"genres\"],  # Convert '|' separated genres into ', '\n",
    "        \"mean\": row.get(\"mean\", 0),  # Ensure default value\n",
    "        \"media_type\": row.get(\"media_type\", \"unknown\"),  # Default if missing\n",
    "        \"status\": row.get(\"status\", \"unknown\")  # Default if missing\n",
    "    }\n",
    "    for _, row in df1.iterrows()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_copies(new_data):\n",
    "    # Convert 'mean' to numeric, coercing errors to NaN\n",
    "    new_data['mean'] = pd.to_numeric(new_data['mean'], errors='coerce')\n",
    "    \n",
    "    # Helper function to combine text fields by concatenating unique, non-empty entries.\n",
    "    def combine_text(series):\n",
    "        unique_entries = series.dropna().astype(str).str.strip()\n",
    "        unique_entries = unique_entries[unique_entries != \"\"].unique()\n",
    "        return \" \".join(unique_entries)\n",
    "    \n",
    "    # Helper function to combine genres by splitting, deduplicating and rejoining.\n",
    "    def combine_genres(series):\n",
    "        genres_set = set()\n",
    "        for entry in series.dropna():\n",
    "            # Assume genres are separated by comma (after converting from '|')\n",
    "            for genre in str(entry).split(\",\"):\n",
    "                genre = genre.strip()\n",
    "                if genre:\n",
    "                    genres_set.add(genre)\n",
    "        return \", \".join(sorted(genres_set))\n",
    "    \n",
    "    # Group by title and combine the rows using aggregation functions.\n",
    "    combined = new_data.groupby(\"title\", as_index=False).agg({\n",
    "        \"id\": lambda x: \", \".join(x.astype(str).unique()),\n",
    "        \"alternative_titles\": combine_text,\n",
    "        \"synopsis\": combine_text,\n",
    "        \"genres\": combine_genres,\n",
    "        \"mean\": \"mean\",\n",
    "        \"media_type\": lambda x: \", \".join(x.dropna().unique()),\n",
    "        \"status\": lambda x: \", \".join(x.dropna().unique())\n",
    "    })\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combine_copies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(df):\n",
    "    # Combine the desired fields into a single text string for each row\n",
    "    def combine_fields(row):\n",
    "        # Convert each field to a string, applying defaults as needed\n",
    "        title = str(row.get(\"title\", \"\"))\n",
    "        alternative_titles = str(row.get(\"alternative_titles\", \"\"))\n",
    "        synopsis = str(row.get(\"synopsis\", \"\"))\n",
    "        # Convert '|' separated genres to a comma-separated string\n",
    "        genres = str(row.get(\"genres\", \"\")).replace(\"|\", \", \")\n",
    "        # Convert numeric mean to string, defaulting to 0 if missing\n",
    "        mean = str(row.get(\"mean\", 0))\n",
    "        media_type = str(row.get(\"media_type\", \"unknown\"))\n",
    "        status = str(row.get(\"status\", \"unknown\"))\n",
    "        # Combine all fields into one string\n",
    "        return \" \".join([title, alternative_titles, synopsis, genres, mean, media_type, status])\n",
    "    \n",
    "    # Create a new column 'combined' that contains the merged text from all fields\n",
    "    df[\"combined\"] = df.apply(combine_fields, axis=1)\n",
    "    \n",
    "    # Use TfidfVectorizer on the combined text\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"combined\"])\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    return similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = compute_similarity(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_anime(user_favorites, df, similarity_matrix):\n",
    "    recommended = set()\n",
    "    for anime_title in user_favorites:\n",
    "        if anime_title in df[\"title\"].values:\n",
    "            # Set na=False to handle NaN values in the title column\n",
    "            idx = df[df[\"title\"].str.contains(anime_title, na=False)].index[0]\n",
    "            similar_indices = similarity_matrix[idx].argsort()[-6:-1][::-1]\n",
    "            recommended.update(df.iloc[similar_indices][\"title\"].values)\n",
    "    return list(recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anime_titles():\n",
    "    return df[\"title\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user_favorites):\n",
    "    recommendations = recommend_anime(user_favorites, df, similarity_matrix)\n",
    "    return \"\\n\".join(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_list(selected_anime, user_list):\n",
    "    if selected_anime and selected_anime not in user_list:\n",
    "        user_list.append(selected_anime)\n",
    "    return user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_list():\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Anime Recommendation System\")\n",
    "    \n",
    "    anime_dropdown = gr.Dropdown(get_anime_titles(), label=\"Select an anime to add to your list\")\n",
    "    user_list = gr.State([])  # Holds the list of selected anime\n",
    "    anime_list_display = gr.Textbox(label=\"Your Anime List\", interactive=False)\n",
    "    \n",
    "    add_button = gr.Button(\"Add to List\")\n",
    "    clear_button = gr.Button(\"Clear List\")\n",
    "    \n",
    "    recommend_button = gr.Button(\"Get Recommendations\")\n",
    "    recommendations_output = gr.Textbox(label=\"Recommended Anime\")\n",
    "    \n",
    "    add_button.click(add_to_list, inputs=[anime_dropdown, user_list], outputs=user_list)\n",
    "    add_button.click(lambda x: \"\\n\".join(x), inputs=user_list, outputs=anime_list_display)\n",
    "    \n",
    "    clear_button.click(clear_list, outputs=user_list)\n",
    "    clear_button.click(lambda: \"\", outputs=anime_list_display)\n",
    "    \n",
    "    recommend_button.click(recommend, inputs=user_list, outputs=recommendations_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Error code: 400 - {'error': {'code': 'billing_hard_limit_reached', 'message': 'Billing hard limit has been reached', 'param': None, 'type': 'invalid_request_error'}}\n",
      "Image generation failed.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "OPENAI_API_KEY = 'OPEN-AI-KEY'\n",
    "load_dotenv()\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "def generate_image(prompt, model=\"dall-e-3\", n=1, size=\"1024x1024\", quality=\"standard\"):\n",
    "    try:\n",
    "        response = openai.images.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            n=n,\n",
    "            size=size,\n",
    "            quality=quality,\n",
    "        )\n",
    "        image_url = response.data[0].url\n",
    "        return image_url\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_prompt = input(\"Enter a text prompt for image generation: \")\n",
    "    if image_prompt:\n",
    "        image_url = generate_image(image_prompt)\n",
    "        if image_url:\n",
    "            print(\"Image URL:\", image_url)\n",
    "        else:\n",
    "            print(\"Image generation failed.\")\n",
    "    else:\n",
    "        print(\"Please enter a valid prompt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Anime-style illustration of two guys. The first has black hair and brown eyes. The second has black hair, blue eyes, and glasses. Both are standing next to each other in a casual, friendly pose, vibrant colors, soft shading, and detailed background.',: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\billy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.28.0)\n",
      "Collecting openai\n",
      "  Using cached openai-1.66.3-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\billy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\billy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\billy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\billy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\billy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\billy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\billy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\billy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\billy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\billy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\billy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\billy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\billy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\billy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\billy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Using cached openai-1.66.3-py3-none-any.whl (567 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.28.0\n",
      "    Uninstalling openai-0.28.0:\n",
      "      Successfully uninstalled openai-0.28.0\n",
      "Successfully installed openai-1.66.3\n"
     ]
    }
   ],
   "source": [
    "! pip install openai --upgrade"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
